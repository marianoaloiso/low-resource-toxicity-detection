model_name: "ai4bharat/indic-bert"
num_labels: 2
max_length: 512
learning_rate: !!float 2e-5
batch_size: 8
num_epochs: 1
r: 16 # Number of attention heads
lora_alpha: 32 # Number of latent vectors
lora_dropout: 0.1
target_modules: ["query", "key", "value", "dense"]
bias: "none"
task_type: "SEQ_CLS"